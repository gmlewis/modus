// Copyright 2024 Hypermode Inc.
// Licensed under the terms of the Apache License, Version 2.0
// See the LICENSE file that accompanied this code for further details.
//
// SPDX-FileCopyrightText: 2024 Hypermode Inc. <hello@hypermode.com>
// SPDX-License-Identifier: Apache-2.0

///| Provides input and output types that conform to the OpenAI Chat API,
/// as described in the [API Reference] docs.
///
/// [API Reference]: https://platform.openai.com/docs/api-reference/chat
pub(all) type ChatModel @models.ModelBase derive(Show, Eq, FromJson, ToJson)

///|
pub impl @models.Model for ChatModel with info(self) {
  self.info()
}

///|
pub fn invoke(
  self : ChatModel,
  input : ChatModelInput
) -> ChatModelOutput!Error {
  let input_json = input.to_json()
  let output_json = self._.invoke!(input_json)
  // println("got output_json=\{output_json}")
  @json.from_json!(output_json)
}

// type ChatModel struct {
// 	chatModelBase
// }
//
// type chatModelBase = models.ModelBase[ChatModelInput, ChatModelOutput]

///| The input object for the OpenAI Chat API.
pub(all) struct ChatModelInput {
  // The name of the model to use for the chat.
  //
  // Must be the exact string expected by the model provider.
  // For example, "gpt-3.5-turbo".
  model : String

  // The list of messages to send to the chat model.
  messages : Array[&RequestMessage]

  // Number between -2.0 and 2.0.
  //
  // Positive values penalize new tokens based on their existing frequency in the text so far,
  // decreasing the model's likelihood to repeat the same line verbatim.
  frequency_penalty : Double?

  // Modifies the likelihood of specified tokens appearing in the completion.
  //
  // Accepts an object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
  // Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model,
  // but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban
  // or exclusive selection of the relevant token.
  logit_bias : Map[String, Double]?

  // Whether to return log probabilities of the output tokens or not,
  //
  // If true, returns the log probabilities of each output token returned in the content of message.
  logprobs : Bool?

  // An integer between 0 and 20 specifying the number of most likely tokens to return at each token position,
  // each with an associated log probability. [Logprobs] must be set to true if this parameter is used.
  top_logprobs : Int?

  // The maximum number of tokens to generate in the chat completion.
  //
  // The default (0) is equivalent to 4096.
  max_tokens : Int?

  // The number of completions to generate for each prompt.
  //
  // The default (0) is equivalent to 1.
  n : Int?

  // Number between -2.0 and 2.0.
  //
  // Positive values penalize new tokens based on whether they appear in the text so far,
  // increasing the model's likelihood to talk about new topics.
  presence_penalty : Double?

  // Specifies the requested format for the response.
  //  - ResponseFormatText requests a plain text string.
  //  - ResponseFormatJson requests a JSON object.
  //  - ResponseFormatjson_schema requests a JSON object that conforms to the provided JSON schema.
  //
  // The default is ResponseFormatText.
  response_format : ResponseFormat

  // If specified, the model will make a best effort to sample deterministically,
  // such that repeated requests with the same seed and parameters should return
  // the same result.
  //
  // Determinism is not guaranteed, and you should use the SystemFingerprint response
  // parameter to monitor changes in the backend.
  //
  // The default (0) is equivalent to a random seed.
  seed : Int?

  // Specifies the latency tier to use for processing the request.
  //  - ServiceTierAuto utilizes scale tier credits until they are exhausted.
  //  - ServiceTierDefault processes the request using the default OpenAI service
  //    tier with a lower uptime SLA and no latency guarantee.
  //
  // The default is ServiceTierAuto.
  service_tier : ServiceTier?

  // Up to 4 sequences where the API will stop generating further tokens.
  stop : Array[String]

  // A number between 0.0 and 2.0 that controls the sampling temperature.
  //
  // Higher values like 0.8 will make the output more random, while lower values
  // like 0.2 will make it more focused and deterministic.
  //
  // We generally recommend altering this or TopP but not both.
  //
  // The default value is 1.0.
  mut temperature : Double

  // An alternative to sampling with temperature, called nucleus sampling, where the model
  // considers the results of the tokens with TopP probability mass.
  //
  // For example, 0.1 means only the tokens comprising the top 10% probability mass are considered.
  //
  // We generally recommend altering this or Temperature but not both.
  //
  // The default value is 1.0.
  top_p : Double?

  // A list of tools the model may call. Currently, only functions are supported as a tool.
  // Use this to provide a list of functions the model may generate JSON inputs for.
  // A max of 128 functions are supported.
  tools : Array[Tool]

  // Controls which (if any) tool is called by the model.
  //  - ToolChoiceNone means the model will not call any tool and instead generates a message.
  //  - ToolChoiceAuto means the model can pick between generating a message or calling one or more tools.
  //  - ToolChoiceRequired means the model must call one or more tools.
  //  - ToolChoiceFunction(name) forces the model to call a specific tool.
  //
  // The default is ToolChoiceAuto when tools are present, and ToolChoiceNone otherwise.
  tool_choice : ToolChoice

  // Whether to enable parallel function calling during tool use.
  //
  // The default is true.
  parallel_tool_calls : Bool?

  // The user ID to associate with the request, as described in the [documentation].
  // If not specified, the request will be anonymous.
  //
  // [documentation]: https://platform.openai.com/docs/guides/safety-best-practices/end-user-ids
  user : String?
} derive(ToJson)

///| An object specifying which tool the model should call.
pub(all) struct ToolChoice {
  // The type of tool to call.
  type_ : String

  // The function to call.
  function : Function?
} derive(Show, Eq, FromJson)

///|
pub impl ToJson for ToolChoice with to_json(_self) {
  // let type_ = self.type_.to_json()
  // match self.function {
  //   None => { "type": type_, "function": Null }
  //   Some(func) => { "type": type_, "function": func.to_json() }
  // }
  // TODO: Fix this
  "auto".to_json()
}

///|
pub(all) struct Function {
  // The name of the function to call.
  name : String
} derive(Show, Eq, FromJson, ToJson)

///| Directs the model to not call any tool and instead generates a message.
pub let tool_choice_none : ToolChoice = { type_: "none", function: None }

///| Directs the model to pick between generating a message or calling one or more tools.
pub let tool_choice_auto : ToolChoice = { type_: "auto", function: None }

///| Directs that the model must call one or more tools.
pub let tool_choice_required : ToolChoice = {
  type_: "required",
  function: None,
}

///| Forces the model to call a specific tool.
pub fn tool_choice_function(name : String) -> ToolChoice {
  { type_: "function", function: Some({ name, }) }
}

///| The output object for the OpenAI Chat API.
pub(all) struct ChatModelOutput {
  // A unique identifier for the chat completion.
  id : String

  // The name of the output object type returned by the API.
  // This will always be "chat.completion".
  object : String

  // A list of chat completion choices. Can be more than one if n is greater than 1 in the input options.
  choices : Array[Choice]

  // The Unix timestamp (in seconds) of when the chat completion was created.
  created : Int

  // The name of the model used to generate the chat.
  // In most cases, this will match the requested model field in the input.
  model : String

  // The service tier used for processing the request.
  //
  // This field is only included if the ServiceTier parameter is specified in the request.
  service_tier : ServiceTier?

  // This fingerprint represents the OpenAI backend configuration that the model runs with.
  //
  // Can be used in conjunction with the Seed request parameter to understand when backend changes
  // have been made that might impact determinism.
  // system_fingerprint : String  // TODO: got `"system_fingerprint": null` in the response.

  // The usage statistics for the request.
  usage : Usage
} derive(Show, Eq, FromJson, ToJson)
// type Message interface {
// 	isMessage()
// }

///| An interface to any message object.
pub trait RequestMessage: ToJson {
  role(Self) -> String
}

///| A system message object.
pub(all) struct SystemMessage {
  /// The role of the author of this message.
  role : String

  /// The content of the message.
  content : String

  /// An optional name for the participant.
  /// Provides the model information to differentiate between participants of the same role.
  name : String?
} derive(Show, Eq, FromJson, ToJson)

///|
pub impl RequestMessage for SystemMessage with role(self) {
  self.role
}

///| Creates a new system message object.
pub fn SystemMessage::new(content : String) -> SystemMessage {
  { role: "system", content, name: None }
}

///| A user message object.
pub(all) struct UserMessage {
  /// The role of the author of this message.
  role : String

  /// The content of the message.
  content : String

  /// An optional name for the participant.
  /// Provides the model information to differentiate between participants of the same role.
  name : String?
} derive(Show, Eq, FromJson, ToJson)

///|
pub impl RequestMessage for UserMessage with role(self) {
  self.role
}

///| Creates a new user message object.
pub fn UserMessage::new(content : String) -> UserMessage {
  { role: "user", content, name: None }
}

///| An assistant message object.
pub(all) struct AssistantMessage {
  /// The role of the author of this message.
  role : String

  /// The content of the message.
  content : String

  /// An optional name for the participant.
  /// Provides the model information to differentiate between participants of the same role.
  name : String?

  /// The tool calls generated by the model, such as function calls.
  tool_calls : Array[ToolCall]
} derive(Show, Eq, FromJson, ToJson)

///|
pub impl RequestMessage for AssistantMessage with role(self) {
  self.role
}

///| Creates a new assistant message object.
pub fn AssistantMessage::new(
  content : String,
  tool_calls~ : Array[ToolCall] = []
) -> AssistantMessage {
  { role: "assistant", content, name: None, tool_calls }
}

///| A tool message object.
pub(all) struct ToolMessage {
  /// The role of the author of this message.
  role : String

  /// The content of the message.
  content : String

  /// The tool call that this message is responding to.
  tool_call_id : String
} derive(Show, Eq, FromJson, ToJson)

///|
pub impl RequestMessage for ToolMessage with role(self) {
  self.role
}

///| Creates a new tool message object.
pub fn ToolMessage::new(content : String, tool_call_id : String) -> ToolMessage {
  { role: "tool", content, tool_call_id }
}

///| A chat completion message generated by the model.
pub(all) struct CompletionMessage {
  /// The role of the author of this message.
  role : String

  /// The content of the message.
  content : String

  /// The refusal message generated by the model.
  refusal : String?
} derive(Show, Eq, FromJson, ToJson)

///|
pub impl RequestMessage for CompletionMessage with role(self) {
  self.role
}

///| A tool call object that the model may generate.
pub(all) struct ToolCall {
  /// The ID of the tool call.
  id : String

  /// The type of the tool. Currently, only `"function"` is supported.
  type_ : String

  /// The function that the model called.
  function : FunctionCall
} derive(Show, Eq, FromJson)

///|
pub impl ToJson for ToolCall with to_json(self) {
  {
    "id": self.id.to_json(),
    "type": self.type_.to_json(),
    "function": self.function.to_json(),
  }
}

///| Creates a new tool call object.
pub fn ToolCall::new(
  id : String,
  name : String,
  arguments : String
) -> ToolCall {
  let function = { name, arguments }
  { id, type_: "function", function }
}

///| A function call object that the model may generate.
pub(all) struct FunctionCall {
  /// The name of the function to call.
  name : String

  /// The arguments to call the function with, as generated by the model in JSON format.
  ///
  /// NOTE:
  /// The model does not always generate valid JSON, and may hallucinate parameters not
  /// defined by your function schema. Validate the arguments in your code before calling your function.
  arguments : String
} derive(Show, Eq, FromJson, ToJson)

///| An object specifying the format that the model must output.
pub(all) struct ResponseFormat {
  /// The type of response format.
  type_ : String

  /// The JSON schema to use for the response format.
  json_schema : Json?
} derive(Show, Eq, FromJson)

///|
pub impl ToJson for ResponseFormat with to_json(self) {
  let type_ = self.type_.to_json()
  match self.json_schema {
    None => { "type": type_, "json_schema": Json::null() }
    Some(json_schema) => { "type": type_, "json_schema": json_schema.to_json() }
  }
}

///| Instructs the model to output the response as a plain text string.
/// This is the default response format.
pub let response_format_text : ResponseFormat = {
  type_: "text",
  json_schema: None,
}

///| Instructs the model to output the response as a JSON object.
///  - You must also instruct the model to produce JSON yourself via a system or user message.
///  - Additionally, if you need an array you must ask for an object that wraps the array,
///    because the model will not reliably produce arrays directly (ie., there is no "json_array" option).
pub let response_format_json : ResponseFormat = {
  type_: "json_object",
  json_schema: None,
}

///| Enables Structured Outputs which guarantees the model will match your supplied JSON schema.
///
/// See https://platform.openai.com/docs/guides/structured-outputs
pub fn response_format_json_schema(json_schema : Json) -> ResponseFormat {
  { type_: "json_schema", json_schema: Some(json_schema) }
}

///| The OpenAI service tier used to process the request.
pub(all) enum ServiceTier {
  /// The OpenAI system will utilize scale tier credits until they are exhausted.
  Auto
  /// The request will be processed using the default OpenAI service tier with a lower
  /// uptime SLA and no latency guarantee.
  Default
} derive(Show, Eq, FromJson, ToJson)

///| A completion choice object returned in the response.
pub(all) struct Choice {
  /// The reason the model stopped generating tokens.
  ///
  /// Possible values are:
  ///  - "stop" if the model hit a natural stop point or a provided stop sequence
  ///  - "length" if the maximum number of tokens specified in the request was reached
  ///  - "content_filter" if content was omitted due to a flag from content filters
  ///  - "tool_calls" if the model called a tool
  finish_reason : String

  /// The index of the choice in the list of choices.
  index : Int

  /// A chat completion message generated by the model.
  message : CompletionMessage

  /// Log probability information for the choice.
  // logprobs : Logprobs  // TODO - found `"logprobs": null` in the response.
} derive(Show, Eq, FromJson, ToJson)

///| Log probability information for a choice.
pub(all) struct Logprobs {
  /// A list of message content tokens with log probability information.
  content : Array[LogprobsContent]
} derive(Show, Eq, FromJson, ToJson)

///| Log probability information for a message content token.
pub(all) struct LogprobsContent {
  /// The token.
  token : String

  /// The log probability of this token, if it is within the top 20 most likely tokens.
  /// Otherwise, the value -9999.0 is used to signify that the token is very unlikely.
  logprob : Double

  /// A list of integers representing the UTF-8 bytes representation of the token.
  ///
  /// Useful in instances where characters are represented by multiple tokens and their byte
  /// representations must be combined to generate the correct text representation.
  /// Can be nil if there is no bytes representation for the token.
  bytes : Json // TODO: Bytes

  /// List of the most likely tokens and their log probability, at this token position.
  /// In rare cases, there may be fewer than the number of requested TopLogprobs returned.
  top_logprobs : Array[LogprobsContentObject]
} derive(Show, Eq, FromJson, ToJson)

///| Log probability information for the most likely tokens at a given position.
pub(all) struct LogprobsContentObject {
  /// The token.
  token : String

  /// The log probability of this token, if it is within the top 20 most likely tokens.
  /// Otherwise, the value -9999.0 is used to signify that the token is very unlikely.
  logprob : Double

  /// A list of integers representing the UTF-8 bytes representation of the token.
  ///
  /// Useful in instances where characters are represented by multiple tokens and their byte
  /// representations must be combined to generate the correct text representation.
  /// Can be nil if there is no bytes representation for the token.
  bytes : Json // TODO: Bytes
} derive(Show, Eq, FromJson, ToJson)

///| A tool object that the model may call.
pub(all) struct Tool {
  /// The type of the tool. Currently, only `"function"` is supported.
  type_ : String

  /// The definition of the function.
  function : FunctionDefinition
} derive(Show, Eq, FromJson, ToJson)

///| Creates a new tool object.
pub fn Tool::new(name : String) -> Tool {
  { type_: "function", function: FunctionDefinition::new(name) }
}

///| The definition of a function that can be called by the model.
pub(all) struct FunctionDefinition {
  /// The name of the function to be called.
  ///
  /// Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
  name : String

  /// An optional description of what the function does, used by the model to choose when and how to call the function.
  description : String?

  /// Whether to enable strict schema adherence when generating the function call.
  ///
  /// If set to true, the model will follow the exact schema defined in the parameters field.
  ///
  /// The default is false.
  ///
  /// See https://platform.openai.com/docs/guides/function-calling
  ///
  /// NOTE:
  /// In order to guarantee strict schema adherence, disable parallel function calls
  /// by setting Paralleltool_calls to false.
  ///
  /// See https://platform.openai.com/docs/guides/function-calling/parallel-function-calling-and-structured-outputs
  strict : Bool?

  /// The parameters the functions accepts, described as a JSON Schema object.
  ///
  /// See https://platform.openai.com/docs/guides/function-calling
  parameters : Json?
} derive(Show, Eq, FromJson, ToJson)

///|
pub fn FunctionDefinition::new(name : String) -> FunctionDefinition {
  { name, description: None, strict: None, parameters: None }
}

///| Creates an input object for the OpenAI Chat API.
pub fn create_input(
  self : ChatModel,
  messages : Array[&RequestMessage]
) -> ChatModelInput {
  let info = self._.info()
  let model = info.full_name
  {
    model,
    messages,
    frequency_penalty: None,
    logit_bias: None,
    logprobs: None,
    top_logprobs: None,
    max_tokens: None,
    n: None,
    presence_penalty: None,
    response_format: response_format_text,
    seed: None,
    service_tier: Some(ServiceTier::Auto),
    stop: [],
    temperature: 1.0,
    top_p: None,
    tools: [],
    tool_choice: tool_choice_auto,
    parallel_tool_calls: None,
    user: None,
  }
}

// func (mi *ChatModelInput) MarshalJSON() ([]byte, error) {
//
// 	type alias ChatModelInput
// 	b, err := utils.JsonSerialize(alias(*mi))
// 	if err != nil {
// 		return nil, err
// 	}
//
/// omit default response_format
// 	if t := mi.ResponseFormat.Type; t == "" || t == "text" {
// 		b, err = sjson.DeleteBytes(b, "response_format")
// 		if err != nil {
// 			return nil, err
// 		}
// 	}
//
/// omit default service_tier
// 	if mi.ServiceTier == ServiceTierAuto {
// 		b, err = sjson.DeleteBytes(b, "service_tier")
// 		if err != nil {
// 			return nil, err
// 		}
// 	}
//
/// omit default temperature
// 	if mi.Temperature == 1.0 {
// 		b, err = sjson.DeleteBytes(b, "temperature")
// 		if err != nil {
// 			return nil, err
// 		}
// 	}
//
/// omit default top_p
// 	if mi.TopP == 1.0 {
// 		b, err = sjson.DeleteBytes(b, "top_p")
// 		if err != nil {
// 			return nil, err
// 		}
// 	}
//
/// don't send parallel_tool_calls if tools are not present
// 	if len(mi.Tools) == 0 {
// 		b, err = sjson.DeleteBytes(b, "parallel_tool_calls")
// 		if err != nil {
// 			return nil, err
// 		}
// 	}
//
/// remove empty tool_choice
// 	if mi.ToolChoice.Type == "" {
// 		b, err = sjson.DeleteBytes(b, "tool_choice")
// 		if err != nil {
// 			return nil, err
// 		}
// 	}
//
// 	return b, nil
// }
//
